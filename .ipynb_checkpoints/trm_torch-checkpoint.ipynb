{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Recursive Model (TRM) - PyTorch Implementation\n",
    "\n",
    "based on: \"less is more: recursive reasoning with tiny networks\"\n",
    "\n",
    "**key architecture:**\n",
    "- single tiny 2-layer network\n",
    "- latent recursion: z = net(x, y, z) for n iterations\n",
    "- answer update: y = net(y, z)\n",
    "- deep supervision: carry (y, z) across supervision steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"character-level dataset for language modeling\"\"\"\n",
    "    \n",
    "    def __init__(self, text, seq_len, char_to_idx=None):\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # build vocab if not provided\n",
    "        if char_to_idx is None:\n",
    "            chars = sorted(set(text))\n",
    "            self.char_to_idx = {c: i for i, c in enumerate(chars)}\n",
    "            self.idx_to_char = {i: c for c, i in self.char_to_idx.items()}\n",
    "        else:\n",
    "            self.char_to_idx = char_to_idx\n",
    "            self.idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
    "        \n",
    "        self.vocab_size = len(self.char_to_idx)\n",
    "        \n",
    "        # encode text\n",
    "        self.data = torch.tensor([self.char_to_idx[c] for c in text], dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_len]\n",
    "        y = self.data[idx + 1:idx + self.seq_len + 1]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"root mean square layer normalization\"\"\"\n",
    "    \n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        return (x / rms) * self.weight\n",
    "\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"swiglu feedforward: swish(x @ w_gate) * (x @ w_up) @ w_down\"\"\"\n",
    "    \n",
    "    def __init__(self, dim, hidden_dim, bias=False):\n",
    "        super().__init__()\n",
    "        self.w_gate = nn.Linear(dim, hidden_dim, bias=bias)\n",
    "        self.w_up = nn.Linear(dim, hidden_dim, bias=bias)\n",
    "        self.w_down = nn.Linear(hidden_dim, dim, bias=bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gate = F.silu(self.w_gate(x))  # silu = swish\n",
    "        up = self.w_up(x)\n",
    "        return self.w_down(gate * up)\n",
    "\n",
    "\n",
    "class MLPMixer(nn.Module):\n",
    "    \"\"\"mlp applied on sequence dimension (token mixing)\"\"\"\n",
    "    \n",
    "    def __init__(self, seq_len, hidden_mult=1):\n",
    "        super().__init__()\n",
    "        hidden = seq_len * hidden_mult\n",
    "        self.fc1 = nn.Linear(seq_len, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, seq_len)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq, dim]\n",
    "        x = x.transpose(1, 2)  # [batch, dim, seq]\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x.transpose(1, 2)  # [batch, seq, dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRMBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    single trm block: norm -> mixer -> norm -> swiglu ffn\n",
    "    with residual connections\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim, hidden_dim, seq_len, use_attention=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(dim)\n",
    "        self.norm2 = RMSNorm(dim)\n",
    "        \n",
    "        if use_attention:\n",
    "            # simple single-head attention\n",
    "            self.mixer = nn.MultiheadAttention(dim, num_heads=4, batch_first=True, bias=False)\n",
    "            self.use_attention = True\n",
    "        else:\n",
    "            self.mixer = MLPMixer(seq_len)\n",
    "            self.use_attention = False\n",
    "        \n",
    "        self.ffn = SwiGLU(dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # mixer block\n",
    "        h = self.norm1(x)\n",
    "        if self.use_attention:\n",
    "            h, _ = self.mixer(h, h, h, need_weights=False)\n",
    "        else:\n",
    "            h = self.mixer(h)\n",
    "        x = x + h\n",
    "        \n",
    "        # ffn block\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trm network (core recursive network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRMNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    tiny recursive network - single network for both:\n",
    "    - latent update: z = net(concat(x, y, z))\n",
    "    - answer update: y = net(concat(y, z))\n",
    "    \n",
    "    task is differentiated by presence/absence of x\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, dim, hidden_dim, seq_len, num_layers=2, use_attention=False):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # embeddings\n",
    "        self.token_embed = nn.Embedding(vocab_size, dim)\n",
    "        \n",
    "        # learnable initial y and z\n",
    "        self.y_init = nn.Parameter(torch.zeros(1, seq_len, dim))\n",
    "        self.z_init = nn.Parameter(torch.zeros(1, seq_len, dim))\n",
    "        \n",
    "        # input projections\n",
    "        # latent: concat(x, y, z) -> 3*dim -> dim\n",
    "        # answer: concat(y, z) -> 2*dim -> dim\n",
    "        self.proj_latent = nn.Linear(3 * dim, dim, bias=False)\n",
    "        self.proj_answer = nn.Linear(2 * dim, dim, bias=False)\n",
    "        \n",
    "        # transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TRMBlock(dim, hidden_dim, seq_len, use_attention)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # output head\n",
    "        self.output_norm = RMSNorm(dim)\n",
    "        self.output_head = nn.Linear(dim, vocab_size, bias=False)\n",
    "        \n",
    "        # q head for halting probability\n",
    "        self.q_head = nn.Linear(dim, 1, bias=False)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight, std=0.02)\n",
    "    \n",
    "    def forward_latent(self, x, y, z):\n",
    "        \"\"\"z_new = net(concat(x, y, z))\"\"\"\n",
    "        h = torch.cat([x, y, z], dim=-1)\n",
    "        h = self.proj_latent(h)\n",
    "        for block in self.blocks:\n",
    "            h = block(h)\n",
    "        return h\n",
    "    \n",
    "    def forward_answer(self, y, z):\n",
    "        \"\"\"y_new = net(concat(y, z))\"\"\"\n",
    "        h = torch.cat([y, z], dim=-1)\n",
    "        h = self.proj_answer(h)\n",
    "        for block in self.blocks:\n",
    "            h = block(h)\n",
    "        return h\n",
    "    \n",
    "    def get_logits(self, y):\n",
    "        \"\"\"convert embedded y to vocab logits\"\"\"\n",
    "        return self.output_head(self.output_norm(y))\n",
    "    \n",
    "    def get_halt_prob(self, y):\n",
    "        \"\"\"halting probability from pooled y\"\"\"\n",
    "        y_pooled = y.mean(dim=1)\n",
    "        return torch.sigmoid(self.q_head(y_pooled).squeeze(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## full trm model with recursion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRM(nn.Module):\n",
    "    \"\"\"\n",
    "    tiny recursive model - full model with recursive reasoning\n",
    "    \n",
    "    algorithm (from paper pseudocode):\n",
    "    1. latent_recursion: for n steps, z = net(x, y, z); then y = net(y, z)\n",
    "    2. deep_recursion: T-1 times without grad, 1 time with grad\n",
    "    3. deep_supervision: repeat for N_sup steps, detaching y,z between steps\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, dim=128, hidden_dim=256, seq_len=32,\n",
    "                 n_latent=6, t_recurse=3, n_sup=4, num_layers=2, use_attention=False):\n",
    "        super().__init__()\n",
    "        self.n_latent = n_latent\n",
    "        self.t_recurse = t_recurse\n",
    "        self.n_sup = n_sup\n",
    "        \n",
    "        self.net = TRMNetwork(\n",
    "            vocab_size, dim, hidden_dim, seq_len,\n",
    "            num_layers=num_layers, use_attention=use_attention\n",
    "        )\n",
    "    \n",
    "    def latent_recursion(self, x, y, z):\n",
    "        \"\"\"single recursion block: n latent updates, then 1 answer update\"\"\"\n",
    "        for _ in range(self.n_latent):\n",
    "            z = self.net.forward_latent(x, y, z)\n",
    "        y = self.net.forward_answer(y, z)\n",
    "        return y, z\n",
    "    \n",
    "    def deep_recursion(self, x, y, z):\n",
    "        \"\"\"\n",
    "        T recursions total:\n",
    "        - T-1 without gradients (use torch.no_grad)\n",
    "        - 1 with gradients (backprop through this)\n",
    "        \"\"\"\n",
    "        # T-1 recursions without gradient\n",
    "        with torch.no_grad():\n",
    "            for _ in range(self.t_recurse - 1):\n",
    "                y, z = self.latent_recursion(x, y, z)\n",
    "        \n",
    "        # final recursion with gradient\n",
    "        y, z = self.latent_recursion(x, y, z)\n",
    "        \n",
    "        return y.detach(), z.detach(), y  # return detached for next step, non-detached for loss\n",
    "    \n",
    "    def forward(self, x_idx, return_all_steps=False):\n",
    "        \"\"\"\n",
    "        full forward with deep supervision\n",
    "        \n",
    "        x_idx: [batch, seq] token indices\n",
    "        returns: logits [batch, seq, vocab] from final supervision step\n",
    "        \"\"\"\n",
    "        batch = x_idx.shape[0]\n",
    "        device = x_idx.device\n",
    "        \n",
    "        # embed input\n",
    "        x = self.net.token_embed(x_idx)\n",
    "        \n",
    "        # initialize y, z\n",
    "        y = self.net.y_init.expand(batch, -1, -1)\n",
    "        z = self.net.z_init.expand(batch, -1, -1)\n",
    "        \n",
    "        all_logits = []\n",
    "        \n",
    "        # deep supervision loop\n",
    "        for step in range(self.n_sup):\n",
    "            y_detached, z_detached, y_for_loss = self.deep_recursion(x, y, z)\n",
    "            \n",
    "            logits = self.net.get_logits(y_for_loss)\n",
    "            all_logits.append(logits)\n",
    "            \n",
    "            # use detached for next step\n",
    "            y, z = y_detached, z_detached\n",
    "        \n",
    "        if return_all_steps:\n",
    "            return all_logits\n",
    "        return all_logits[-1]  # return final step logits\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, seed_idx, max_len=100, temperature=1.0):\n",
    "        \"\"\"\n",
    "        generate tokens autoregressively\n",
    "        \n",
    "        seed_idx: [seq_len] starting token indices\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        tokens = seed_idx.tolist() if isinstance(seed_idx, torch.Tensor) else list(seed_idx)\n",
    "        seq_len = self.net.seq_len\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            # prepare input\n",
    "            if len(tokens) < seq_len:\n",
    "                x = [0] * (seq_len - len(tokens)) + tokens\n",
    "            else:\n",
    "                x = tokens[-seq_len:]\n",
    "            \n",
    "            x = torch.tensor([x], dtype=torch.long, device=device)\n",
    "            \n",
    "            # forward\n",
    "            logits = self.forward(x)  # [1, seq, vocab]\n",
    "            \n",
    "            # sample from last position\n",
    "            probs = F.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1).item()\n",
    "            \n",
    "            tokens.append(next_token)\n",
    "        \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"count trainable parameters\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device, accumulation_steps=1, print_every=10):\n",
    "    \"\"\"train for one epoch with optional gradient accumulation\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    batch_losses = []\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    for step, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # forward - get all supervision step logits\n",
    "        all_logits = model(x, return_all_steps=True)\n",
    "\n",
    "        # compute loss over all supervision steps (deep supervision)\n",
    "        loss = 0\n",
    "        for logits in all_logits:\n",
    "            step_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            loss = loss + step_loss\n",
    "        loss = loss / len(all_logits)\n",
    "\n",
    "        # backward with accumulation\n",
    "        loss_scaled = loss / accumulation_steps\n",
    "        loss_scaled.backward()\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # track actual loss (not scaled)\n",
    "        total_loss += loss.item() * y.numel()\n",
    "        total_tokens += y.numel()\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "        # progress logging\n",
    "        if (step + 1) % print_every == 0:\n",
    "            avg_loss = total_loss / total_tokens\n",
    "            print(f\"  batch {step + 1:3d}/{num_batches} | loss: {loss.item():.4f} | avg: {avg_loss:.4f}\")\n",
    "\n",
    "    return total_loss / total_tokens, batch_losses\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"evaluate model, return loss and accuracy\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), reduction='sum')\n",
    "        \n",
    "        preds = logits.argmax(dim=-1)\n",
    "        correct = (preds == y).sum().item()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_correct += correct\n",
    "        total_tokens += y.numel()\n",
    "    \n",
    "    return total_loss / total_tokens, total_correct / total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualization utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMonitor:\n",
    "    \"\"\"track and visualize training metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.test_accs = []\n",
    "        self.batch_losses = []\n",
    "    \n",
    "    def update(self, train_loss, test_loss, test_acc, batch_losses=None):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.test_losses.append(test_loss)\n",
    "        self.test_accs.append(test_acc)\n",
    "        if batch_losses is not None:\n",
    "            self.batch_losses.extend(batch_losses)\n",
    "    \n",
    "    def plot(self, live=False):\n",
    "        if live:\n",
    "            clear_output(wait=True)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        \n",
    "        # epoch losses\n",
    "        axes[0].plot(self.train_losses, label='train loss', marker='o')\n",
    "        axes[0].plot(self.test_losses, label='test loss', marker='s')\n",
    "        axes[0].set_xlabel('epoch')\n",
    "        axes[0].set_ylabel('loss')\n",
    "        axes[0].set_title('epoch losses')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # test accuracy\n",
    "        axes[1].plot(self.test_accs, label='test accuracy', marker='o', color='green')\n",
    "        axes[1].set_xlabel('epoch')\n",
    "        axes[1].set_ylabel('accuracy')\n",
    "        axes[1].set_title('test accuracy')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # batch losses (smoothed)\n",
    "        if len(self.batch_losses) > 0:\n",
    "            window = min(50, len(self.batch_losses) // 10 + 1)\n",
    "            smoothed = np.convolve(self.batch_losses, np.ones(window)/window, mode='valid')\n",
    "            axes[2].plot(smoothed, alpha=0.7, color='blue')\n",
    "            axes[2].set_xlabel('batch')\n",
    "            axes[2].set_ylabel('loss')\n",
    "            axes[2].set_title(f'batch losses (smoothed, window={window})')\n",
    "            axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def print_summary(self):\n",
    "        if len(self.test_losses) > 0:\n",
    "            best_epoch = np.argmin(self.test_losses)\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"training summary\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"best epoch: {best_epoch + 1}\")\n",
    "            print(f\"best test loss: {self.test_losses[best_epoch]:.4f}\")\n",
    "            print(f\"best test accuracy: {self.test_accs[best_epoch]:.4f}\")\n",
    "            print(f\"final test loss: {self.test_losses[-1]:.4f}\")\n",
    "            print(f\"final test accuracy: {self.test_accs[-1]:.4f}\")\n",
    "            print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load shakespeare text\n",
    "filepath = \"shakespeare.txt\"\n",
    "with open(filepath, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"loaded {len(text):,} characters\")\n",
    "print(f\"\\nfirst 500 characters:\")\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "seq_len = 32\n",
    "batch_size = 32\n",
    "\n",
    "# create dataset\n",
    "dataset = CharDataset(text, seq_len)\n",
    "vocab_size = dataset.vocab_size\n",
    "\n",
    "print(f\"vocab size: {vocab_size}\")\n",
    "print(f\"unique characters: {sorted(dataset.char_to_idx.keys())}\")\n",
    "\n",
    "# train/test split\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"\\ntrain samples: {train_size:,}\")\n",
    "print(f\"test samples: {test_size:,}\")\n",
    "print(f\"batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "dim = 64\n",
    "hidden_dim = 128\n",
    "n_latent = 3\n",
    "t_recurse = 2\n",
    "n_sup = 2\n",
    "num_layers = 2\n",
    "use_attention = False  # mlp mixer (paper shows it works better for small fixed context)\n",
    "\n",
    "# create model\n",
    "model = TRM(\n",
    "    vocab_size=vocab_size,\n",
    "    dim=dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    seq_len=seq_len,\n",
    "    n_latent=n_latent,\n",
    "    t_recurse=t_recurse,\n",
    "    n_sup=n_sup,\n",
    "    num_layers=num_layers,\n",
    "    use_attention=use_attention\n",
    ").to(device)\n",
    "\n",
    "num_params = count_parameters(model)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"model architecture\")\n",
    "print(\"=\"*60)\n",
    "print(f\"embedding dim: {dim}\")\n",
    "print(f\"hidden dim: {hidden_dim}\")\n",
    "print(f\"sequence length: {seq_len}\")\n",
    "print(f\"latent recursions: {n_latent}\")\n",
    "print(f\"deep recursions: {t_recurse}\")\n",
    "print(f\"supervision steps: {n_sup}\")\n",
    "print(f\"transformer layers: {num_layers}\")\n",
    "print(f\"use attention: {use_attention}\")\n",
    "print(f\"\\ntotal parameters: {num_params:,}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparameters\n",
    "epochs = 5\n",
    "lr = 1e-3\n",
    "accumulation_steps = 1\n",
    "\n",
    "# optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.1, betas=(0.9, 0.95))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "\n",
    "# training monitor\n",
    "monitor = TrainingMonitor()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"starting training\")\n",
    "print(\"=\"*60)\n",
    "print(f\"epochs: {epochs}\")\n",
    "print(f\"learning rate: {lr}\")\n",
    "print(f\"batch size: {batch_size}\")\n",
    "print(f\"accumulation steps: {accumulation_steps}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nepoch {epoch+1}/{epochs}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # train\n",
    "    train_loss, batch_losses = train_epoch(\n",
    "        model, train_loader, optimizer, device, \n",
    "        accumulation_steps=accumulation_steps, \n",
    "        print_every=5\n",
    "    )\n",
    "    \n",
    "    # evaluate\n",
    "    test_loss, test_acc = evaluate(model, test_loader, device)\n",
    "    \n",
    "    # update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # track metrics\n",
    "    monitor.update(train_loss, test_loss, test_acc, batch_losses)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    # save best model\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        torch.save(model.state_dict(), 'trm_best.pt')\n",
    "        print(f\"  *** saved best model ***\")\n",
    "    \n",
    "    print(f\"\\nepoch {epoch+1} summary:\")\n",
    "    print(f\"  train_loss: {train_loss:.4f}\")\n",
    "    print(f\"  test_loss: {test_loss:.4f}\")\n",
    "    print(f\"  test_acc: {test_acc:.4f}\")\n",
    "    print(f\"  time: {elapsed:.1f}s\")\n",
    "    \n",
    "    # plot live updates\n",
    "    monitor.plot(live=True)\n",
    "\n",
    "print(\"\\ntraining complete!\")\n",
    "monitor.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot final training curves\n",
    "monitor.plot(live=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load best model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "model.load_state_dict(torch.load('trm_best.pt'))\n",
    "\n",
    "# final evaluation\n",
    "print(\"final evaluation on best model...\")\n",
    "test_loss, test_acc = evaluate(model, test_loader, device)\n",
    "print(f\"\\nbest test loss: {test_loss:.4f}\")\n",
    "print(f\"best test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, dataset, seed, max_len=300, temperature=0.8):\n",
    "    \"\"\"generate text from a seed string\"\"\"\n",
    "    # encode seed\n",
    "    seed_idx = [dataset.char_to_idx.get(c, 0) for c in seed]\n",
    "    # pad to seq_len\n",
    "    seed_idx = [0] * (seq_len - len(seed_idx)) + seed_idx\n",
    "    seed_tensor = torch.tensor(seed_idx, dtype=torch.long, device=device)\n",
    "    \n",
    "    # generate\n",
    "    generated_idx = model.generate(seed_tensor, max_len=max_len, temperature=temperature)\n",
    "    generated_text = ''.join([dataset.idx_to_char.get(i, '?') for i in generated_idx])\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate with different seeds\n",
    "seeds = [\"ROMEO:\", \"JULIET:\", \"First Citizen:\", \"KING HENRY:\"]\n",
    "temperature = 0.8\n",
    "max_len = 200\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"text generation examples\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"\\nseed: '{seed}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    generated = generate_text(model, dataset, seed, max_len=max_len, temperature=temperature)\n",
    "    \n",
    "    # find where seed ends in generated text\n",
    "    seed_end = generated.find(seed) + len(seed)\n",
    "    if seed_end > len(seed):\n",
    "        print(generated[seed_end:seed_end+max_len])\n",
    "    else:\n",
    "        print(generated[-max_len:])\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## interactive generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive generation cell - run this to generate with custom seeds\n",
    "custom_seed = \"To be, or not to be\"  # change this!\n",
    "custom_temp = 0.7  # change this (0.1 = conservative, 1.0 = creative)\n",
    "custom_len = 300  # change this\n",
    "\n",
    "print(f\"seed: '{custom_seed}'\")\n",
    "print(f\"temperature: {custom_temp}\")\n",
    "print(f\"max length: {custom_len}\")\n",
    "print(\"\\ngenerated text:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "generated = generate_text(model, dataset, custom_seed, max_len=custom_len, temperature=custom_temp)\n",
    "seed_end = generated.find(custom_seed) + len(custom_seed)\n",
    "if seed_end > len(custom_seed):\n",
    "    print(custom_seed + generated[seed_end:seed_end+custom_len])\n",
    "else:\n",
    "    print(generated[-custom_len:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze model predictions on test samples\n",
    "model.eval()\n",
    "\n",
    "# get a batch from test set\n",
    "test_iter = iter(test_loader)\n",
    "x, y = next(test_iter)\n",
    "x, y = x.to(device), y.to(device)\n",
    "\n",
    "# get predictions\n",
    "with torch.no_grad():\n",
    "    logits = model(x)\n",
    "    preds = logits.argmax(dim=-1)\n",
    "\n",
    "# show first few examples\n",
    "num_examples = 3\n",
    "print(\"=\"*60)\n",
    "print(\"prediction examples\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(num_examples):\n",
    "    input_text = ''.join([dataset.idx_to_char[idx.item()] for idx in x[i]])\n",
    "    target_text = ''.join([dataset.idx_to_char[idx.item()] for idx in y[i]])\n",
    "    pred_text = ''.join([dataset.idx_to_char[idx.item()] for idx in preds[i]])\n",
    "    \n",
    "    # calculate accuracy for this sequence\n",
    "    acc = (preds[i] == y[i]).float().mean().item()\n",
    "    \n",
    "    print(f\"\\nexample {i+1}:\")\n",
    "    print(f\"input:  {input_text}\")\n",
    "    print(f\"target: {target_text}\")\n",
    "    print(f\"pred:   {pred_text}\")\n",
    "    print(f\"accuracy: {acc:.2%}\")\n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
